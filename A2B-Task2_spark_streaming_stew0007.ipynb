{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Streaming application using Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\tWrite code to create a SparkSession, which 1) uses four cores with a proper application name; 2) use the Melbourne timezone; 3) ensure a checkpoint location has been set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    col, from_json, from_unixtime, window, current_timestamp, \n",
    "    expr, year, month, dayofmonth, hour, minute, when, explode, \n",
    "    current_date, sum as spark_sum\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    FloatType, DoubleType, TimestampType, BooleanType, ArrayType\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Configuration:\n",
      "App Name: KafkaSparkStreaming\n",
      "Master: local[6]\n",
      "Timezone: Australia/Melbourne\n",
      "Checkpoint Location: /tmp/spark-checkpoint\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Spark session with the required configurations\n",
    "# Running in local mode with 4 cores, allocating 12GB memory for processing\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSparkStreaming\") \\\n",
    "    .master(\"local[6]\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Australia/Melbourne\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/spark-checkpoint\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify if the Spark session is configured correctly\n",
    "print(\"Spark Session Configuration:\")\n",
    "print(f\"App Name: {spark.sparkContext.appName}\")  # Display the application name\n",
    "print(f\"Master: {spark.sparkContext.master}\")  # Verify where the Spark application is running\n",
    "print(f\"Timezone: {spark.conf.get('spark.sql.session.timeZone')}\")  # Check if timezone is set correctly\n",
    "print(f\"Checkpoint Location: {spark.conf.get('spark.sql.streaming.checkpointLocation')}\")  # Confirm the checkpoint path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tSimilar to assignment 2A, write code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets (e.g. customer, product, category) into data frames. (You can use your code from 2A.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the schema for Customer.csv\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), nullable=False),  # ID for each customer (must be unique)\n",
    "    StructField(\"first_name\", StringType(), nullable=True),  # Customer's first name (optional)\n",
    "    StructField(\"last_name\", StringType(), nullable=True),  # Customer's last name (optional)\n",
    "    StructField(\"username\", StringType(), nullable=True),  # Username chosen by the customer\n",
    "    StructField(\"email\", StringType(), nullable=True),  # Customer's email (could be useful for contact)\n",
    "    StructField(\"gender\", StringType(), nullable=True),  # Gender of the customer\n",
    "    StructField(\"birthdate\", StringType(), nullable=True),  # Customer's birthdate (for age calculations)\n",
    "    StructField(\"first_join_date\", StringType(), nullable=True)  # The date when the customer first joined\n",
    "])\n",
    "\n",
    "# Defining the schema for Category.csv\n",
    "category_schema = StructType([\n",
    "    StructField(\"category_id\", IntegerType(), nullable=False),  # Unique ID representing each category\n",
    "    StructField(\"cat_level1\", StringType(), nullable=True),  # Top-level category (e.g., Electronics)\n",
    "    StructField(\"cat_level2\", StringType(), nullable=True),  # Sub-category (e.g., Mobile Phones)\n",
    "    StructField(\"cat_level3\", StringType(), nullable=True)  # Further breakdown (e.g., Accessories)\n",
    "])\n",
    "\n",
    "# Defining the schema for Browsing_behaviour.csv\n",
    "browsing_behaviour_schema = StructType([\n",
    "    StructField(\"session_id\", StringType(), nullable=False),  # Unique session ID per browsing session\n",
    "    StructField(\"event_type\", StringType(), nullable=True),  # The action user took (e.g., view, click)\n",
    "    StructField(\"event_time\", TimestampType(), nullable=True),  # When the event occurred\n",
    "    StructField(\"traffic_source\", StringType(), nullable=True),  # How user found the website (e.g., Google)\n",
    "    StructField(\"device_type\", StringType(), nullable=True)  # Type of device used (e.g., mobile, desktop)\n",
    "])\n",
    "\n",
    "# Defining the schema for Product.csv\n",
    "product_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), nullable=False),  # ID for each product (primary key)\n",
    "    StructField(\"gender\", StringType(), nullable=True),  # Intended gender for the product (if any)\n",
    "    StructField(\"baseColour\", StringType(), nullable=True),  # Main color of the product\n",
    "    StructField(\"season\", StringType(), nullable=True),  # Productâ€™s season (e.g., Winter collection)\n",
    "    StructField(\"year\", IntegerType(), nullable=True),  # Year the product was released or made\n",
    "    StructField(\"usage\", StringType(), nullable=True),  # Primary use case of the product (optional)\n",
    "    StructField(\"productDisplayName\", StringType(), nullable=True),  # Display name shown to users\n",
    "    StructField(\"category_id\", IntegerType(), nullable=True)  # Reference to product's category\n",
    "])\n",
    "\n",
    "# Defining the schema for Transaction.csv\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"created_at\", TimestampType(), nullable=False),  # Timestamp of the transaction\n",
    "    StructField(\"customer_id\", IntegerType(), nullable=False),  # Customer ID involved in the transaction\n",
    "    StructField(\"transaction_id\", StringType(), nullable=False),  # Unique ID for each transaction\n",
    "    StructField(\"session_id\", StringType(), nullable=True),  # Session linked to the transaction\n",
    "    StructField(\"product_metadata\", StringType(), nullable=True),  # Metadata for products bought\n",
    "    StructField(\"payment_method\", StringType(), nullable=True),  # Payment method used (e.g., card, PayPal)\n",
    "    StructField(\"payment_status\", StringType(), nullable=True),  # Status (e.g., successful, failed)\n",
    "    StructField(\"promo_amount\", FloatType(), nullable=True),  # Discount applied (if any)\n",
    "    StructField(\"promo_code\", StringType(), nullable=True),  # Code used for promotion\n",
    "    StructField(\"shipment_fee\", FloatType(), nullable=True),  # Fee for shipping\n",
    "    StructField(\"shipment_location_lat\", DoubleType(), nullable=True),  # Latitude of the shipping address\n",
    "    StructField(\"shipment_location_long\", DoubleType(), nullable=True),  # Longitude of the shipping address\n",
    "    StructField(\"total_amount\", FloatType(), nullable=True),  # Total value of the transaction\n",
    "    StructField(\"clear_payment\", BooleanType(), nullable=True)  # Whether the payment was completed successfully\n",
    "])\n",
    "\n",
    "# Defining the schema for Customer_session.csv\n",
    "customer_session_schema = StructType([\n",
    "    StructField(\"session_id\", StringType(), nullable=False),  # ID representing the browsing session\n",
    "    StructField(\"customer_id\", IntegerType(), nullable=False)  # Which customer was in the session\n",
    "])\n",
    "\n",
    "# Defining the schema for Fraud_transaction.csv\n",
    "fraud_transaction_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), nullable=False),  # Transaction ID involved in fraud\n",
    "    StructField(\"is_fraud\", BooleanType(), nullable=False)  # Whether the transaction was fraudulent or not\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/student/BIGDATA/LABORATORY/ASSIGNMENTS\n",
      "Customer dataset schema:\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      " |-- first_join_date: string (nullable = true)\n",
      "\n",
      "\n",
      "Category dataset schema:\n",
      "root\n",
      " |-- category_id: integer (nullable = true)\n",
      " |-- cat_level1: string (nullable = true)\n",
      " |-- cat_level2: string (nullable = true)\n",
      " |-- cat_level3: string (nullable = true)\n",
      "\n",
      "\n",
      "Browsing Behaviour dataset schema:\n",
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      "\n",
      "\n",
      "Product dataset schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- baseColour: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- usage: string (nullable = true)\n",
      " |-- productDisplayName: string (nullable = true)\n",
      " |-- category_id: integer (nullable = true)\n",
      "\n",
      "\n",
      "Transaction dataset schema:\n",
      "root\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- product_metadata: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- promo_amount: float (nullable = true)\n",
      " |-- promo_code: string (nullable = true)\n",
      " |-- shipment_fee: float (nullable = true)\n",
      " |-- shipment_location_lat: double (nullable = true)\n",
      " |-- shipment_location_long: double (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      " |-- clear_payment: boolean (nullable = true)\n",
      "\n",
      "\n",
      "Customer Session dataset schema:\n",
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      "\n",
      "\n",
      "Fraud Transaction dataset schema:\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- is_fraud: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "curr_dir = os.getcwd() + '/A2/dataset/dataset'\n",
    "\n",
    "# Reading in all CSV files with their respective schemas\n",
    "cust_data = spark.read.csv(curr_dir + \"/customer.csv\", header=True, schema=customer_schema)\n",
    "cat_data = spark.read.csv(curr_dir + \"/category.csv\", header=True, schema=category_schema)\n",
    "browse_data = spark.read.csv(curr_dir + \"/browsing_behaviour.csv\", header=True, schema=browsing_behaviour_schema)\n",
    "prod_data = spark.read.csv(curr_dir + \"/product.csv\", header=True, schema=product_schema)\n",
    "trans_data = spark.read.csv(curr_dir + \"/transactions.csv\", header=True, schema=transaction_schema)\n",
    "session_data = spark.read.csv(curr_dir + \"/customer_session.csv\", header=True, schema=customer_session_schema)\n",
    "fraud_data = spark.read.csv(curr_dir + \"/fraud_transaction.csv\", header=True, schema=fraud_transaction_schema)\n",
    "\n",
    "# Printing the schema for each DataFrame to make sure they loaded correctly\n",
    "print(\"Customer dataset schema:\")\n",
    "cust_data.printSchema()\n",
    "\n",
    "print(\"\\nCategory dataset schema:\")\n",
    "cat_data.printSchema()\n",
    "\n",
    "print(\"\\nBrowsing Behaviour dataset schema:\")\n",
    "browse_data.printSchema()\n",
    "\n",
    "print(\"\\nProduct dataset schema:\")\n",
    "prod_data.printSchema()\n",
    "\n",
    "print(\"\\nTransaction dataset schema:\")\n",
    "trans_data.printSchema()\n",
    "\n",
    "print(\"\\nCustomer Session dataset schema:\")\n",
    "session_data.printSchema()\n",
    "\n",
    "print(\"\\nFraud Transaction dataset schema:\")\n",
    "fraud_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using the Kafka topics from the producer in Task 1, ingest the streaming data into Spark Streaming, assuming all data comes in the String format. Except for the 'ts' column, you shall receive it as an Int type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created DataFrames for Browsing and Transaction streams!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType\n",
    "\n",
    "# Set Kafka broker configurations\n",
    "hostip = \"kafka\"  # Replace this with the Kafka broker's IP or hostname if needed\n",
    "browsing_topic = 'BrowsingBehaviour-Topic'  # Topic name for browsing events\n",
    "transaction_topic = 'Transaction-Topic'  # Topic name for transaction events\n",
    "\n",
    "# Define the schema for browsing data\n",
    "browsing_schema = StructType([\n",
    "    StructField(\"session_id\", StringType(), True),  # Unique ID for each session\n",
    "    StructField(\"event_type\", StringType(), True),  # Type of event (click, view, etc.)\n",
    "    StructField(\"event_time\", StringType(), True),  # Time when the event occurred (as string)\n",
    "    StructField(\"traffic_source\", StringType(), True),  # Source of the traffic (e.g., Google)\n",
    "    StructField(\"device_type\", StringType(), True),  # Type of device used (e.g., mobile, desktop)\n",
    "    StructField(\"customer_id\", StringType(), True),  # ID of the customer, if available\n",
    "    StructField(\"ts\", IntegerType(), True)  # Unix timestamp for event time\n",
    "])\n",
    "\n",
    "# Define the schema for transaction data\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"created_at\", StringType(), True),  # Transaction creation time (as string)\n",
    "    StructField(\"customer_id\", StringType(), True),  # ID of the customer making the purchase\n",
    "    StructField(\"transaction_id\", StringType(), True),  # Unique ID for each transaction\n",
    "    StructField(\"session_id\", StringType(), True),  # Session linked to the transaction\n",
    "    StructField(\"product_metadata\", StringType(), True),  # Product details (as string)\n",
    "    StructField(\"payment_method\", StringType(), True),  # Payment method used\n",
    "    StructField(\"payment_status\", StringType(), True),  # Status of the payment\n",
    "    StructField(\"promo_amount\", StringType(), True),  # Amount discounted via promo code\n",
    "    StructField(\"promo_code\", StringType(), True),  # Promo code used (if any)\n",
    "    StructField(\"shipment_fee\", StringType(), True),  # Shipping fee for the order\n",
    "    StructField(\"shipment_location_lat\", StringType(), True),  # Latitude of delivery address\n",
    "    StructField(\"shipment_location_long\", StringType(), True),  # Longitude of delivery address\n",
    "    StructField(\"total_amount\", StringType(), True),  # Total transaction amount\n",
    "    StructField(\"clear_payment\", StringType(), True),  # Whether payment was cleared\n",
    "    StructField(\"ts\", IntegerType(), True)  # Unix timestamp for the transaction\n",
    "])\n",
    "\n",
    "# Read browsing data from Kafka\n",
    "browsing_stream_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"subscribe\", browsing_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Read transaction data from Kafka\n",
    "transaction_stream_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"subscribe\", transaction_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse browsing data with the schema\n",
    "browsing_stream_parsed_df = browsing_stream_df \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), browsing_schema).alias(\"browsing_data\")) \\\n",
    "    .select(\"browsing_data.*\")\n",
    "\n",
    "# Parse transaction data with the schema\n",
    "transaction_stream_parsed_df = transaction_stream_df \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), transaction_schema).alias(\"transaction_data\")) \\\n",
    "    .select(\"transaction_data.*\")\n",
    "\n",
    "# Print a success message to confirm dataframes creation\n",
    "print(\"Successfully created DataFrames for Browsing and Transaction streams!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming browsing and transaction data to console...\n"
     ]
    }
   ],
   "source": [
    "# Stream browsing data to console\n",
    "browsing_query = browsing_stream_parsed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "# Stream transaction data to console\n",
    "transaction_query = transaction_stream_parsed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming browsing and transaction data to console...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browsing and Transaction streaming processes stopped.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To stop manually, you can call:\n",
    "browsing_query.stop()\n",
    "transaction_query.stop()\n",
    "\n",
    "print(\"Browsing and Transaction streaming processes stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\tThen, the streaming data format should be transformed into the proper formats following the metadata file schema, similar to assignment 2A. Perform the following tasks:  \n",
    "a)\tFor the 'ts' column, convert it to the timestamp format, we will use it as event_ts.  \n",
    "b)\tIf the data is late for more than 2 minutes, discard it.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'ts' (Unix timestamp) to 'event_ts' and apply watermark for browsing data\n",
    "browsing_stream_parsed_df = browsing_stream_parsed_df \\\n",
    "    .withColumn(\"event_ts\", from_unixtime(col(\"ts\")).cast(\"timestamp\")) \\\n",
    "    .withWatermark(\"event_ts\", \"10 minutes\")  # Watermark to allow streaming joins\n",
    "\n",
    "# Convert 'ts' to 'created_ts' and apply watermark for transaction data\n",
    "transaction_stream_parsed_df = transaction_stream_parsed_df \\\n",
    "    .withColumn(\"event_ts\", from_unixtime(col(\"ts\")).cast(\"timestamp\")) \\\n",
    "    .withWatermark(\"event_ts\", \"10 minutes\")  # Watermark to allow streaming joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task (a): Parse 'ts' to 'event_ts' and 'created_ts' to make them timestamp format\n",
    "browsing_stream_parsed_df = browsing_stream_parsed_df \\\n",
    "    .withColumn(\"event_ts\", from_unixtime(col(\"ts\")).cast(\"timestamp\"))  # Convert 'ts' to 'event_ts'\n",
    "\n",
    "transaction_stream_parsed_df = transaction_stream_parsed_df \\\n",
    "    .withColumn(\"created_ts\", from_unixtime(col(\"ts\")).cast(\"timestamp\"))  # Convert 'ts' to 'created_ts'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task (b):# Filter out late data (more than 2 minutes delay) for both streams\n",
    "browsing_stream_with_watermark = browsing_stream_parsed_df \\\n",
    "    .withColumn(\"current_ts\", current_timestamp()) \\\n",
    "    .filter(expr(\"current_ts - event_ts <= interval 2 minutes\"))\n",
    "\n",
    "transaction_stream_with_watermark = transaction_stream_parsed_df \\\n",
    "    .withColumn(\"current_ts\", current_timestamp()) \\\n",
    "    .filter(expr(\"current_ts - created_ts <= interval 2 minutes\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply watermarks again to both streams (redundant but kept for clarity)\n",
    "browsing_stream_with_watermark = browsing_stream_with_watermark \\\n",
    "    .withColumn(\"event_ts\", from_unixtime(col(\"ts\")).cast(\"timestamp\")) \\\n",
    "    .withWatermark(\"event_ts\", \"10 minutes\")  # Watermark event_ts\n",
    "\n",
    "transaction_stream_with_watermark = transaction_stream_with_watermark \\\n",
    "    .withColumn(\"created_ts\", from_unixtime(col(\"ts\")).cast(\"timestamp\")) \\\n",
    "    .withWatermark(\"created_ts\", \"10 minutes\")  # Watermark created_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the browsing stream to the console\n",
    "browsing_final_stream_df_query = browsing_stream_with_watermark.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "# Output the transaction stream to the console\n",
    "transaction_stream_parsed_df_ts_parsed_query = transaction_stream_with_watermark.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "browsing_final_stream_df_query.stop()\n",
    "transaction_stream_parsed_df_ts_parsed_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\tAggregate the streaming data frames and create features you used in your assignment 2A model.  \n",
    "(note: customer ID has already been included in the stream.) Then, join the static data frames with the streaming data frame as our final data for prediction. Perform data type/column conversion according to your ML model and print out the Schema. (Again, you can reuse code from A2A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- browsing_session_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- ts: integer (nullable = true)\n",
      " |-- event_ts: timestamp (nullable = true)\n",
      " |-- browsing_current_ts: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "browsing_stream_parsed_df_ts_parsed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- product_metadata: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- promo_amount: string (nullable = true)\n",
      " |-- promo_code: string (nullable = true)\n",
      " |-- shipment_fee: string (nullable = true)\n",
      " |-- shipment_location_lat: string (nullable = true)\n",
      " |-- shipment_location_long: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- clear_payment: string (nullable = true)\n",
      " |-- ts: integer (nullable = true)\n",
      " |-- event_ts: timestamp (nullable = true)\n",
      " |-- created_ts: timestamp (nullable = true)\n",
      " |-- current_ts: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_stream_with_watermark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in transaction stream to avoid naming conflicts during join\n",
    "transaction_stream_parsed_df_ts_parsed = transaction_stream_with_watermark \\\n",
    "    .withColumnRenamed(\"session_id\", \"transaction_session_id\") \\\n",
    "    .withColumnRenamed(\"current_ts\", \"transaction_current_ts\") \\\n",
    "    .withColumnRenamed(\"event_ts\", \"transaction_event_ts\") \\\n",
    "    .withColumnRenamed(\"ts\", \"transaction_ts\")  # Rename Unix timestamp for clarity\n",
    "\n",
    "# Rename columns in browsing stream to avoid ambiguity\n",
    "browsing_stream_parsed_df_ts_parsed = browsing_stream_with_watermark \\\n",
    "    .withColumnRenamed(\"session_id\", \"browsing_session_id\") \\\n",
    "    .withColumnRenamed(\"current_ts\", \"browsing_current_ts\")  # Differentiate current timestamps\n",
    "\n",
    "# Create aliases to make joins clearer and easier to read\n",
    "transaction_alias = transaction_stream_parsed_df_ts_parsed.alias(\"transaction\")  \n",
    "browsing_alias = browsing_stream_parsed_df_ts_parsed.alias(\"browsing\")  \n",
    "\n",
    "# Perform an inner join on customer_id and matching timestamps within a 5-minute window\n",
    "joined_stream_df = transaction_alias.join(\n",
    "    browsing_alias,\n",
    "    expr(\"\"\"\n",
    "        transaction.customer_id = browsing.customer_id AND\n",
    "        browsing.event_ts BETWEEN transaction.created_ts - INTERVAL 5 MINUTES\n",
    "        AND transaction.created_ts + INTERVAL 5 MINUTES\n",
    "    \"\"\"),\n",
    "    how=\"inner\"  # Use inner join to keep only matching records\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the streaming data to the console\n",
    "final_stream_df_query = joined_stream_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stream_df_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- transaction_session_id: string (nullable = true)\n",
      " |-- product_metadata: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- promo_amount: string (nullable = true)\n",
      " |-- promo_code: string (nullable = true)\n",
      " |-- shipment_fee: string (nullable = true)\n",
      " |-- shipment_location_lat: string (nullable = true)\n",
      " |-- shipment_location_long: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- clear_payment: string (nullable = true)\n",
      " |-- transaction_ts: integer (nullable = true)\n",
      " |-- transaction_event_ts: timestamp (nullable = true)\n",
      " |-- created_ts: timestamp (nullable = true)\n",
      " |-- transaction_current_ts: timestamp (nullable = false)\n",
      " |-- browsing_session_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- ts: integer (nullable = true)\n",
      " |-- event_ts: timestamp (nullable = true)\n",
      " |-- browsing_current_ts: timestamp (nullable = false)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      " |-- first_join_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_final_stream = joined_stream_df.join(\n",
    "    cust_data,\n",
    "    on=\"customer_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "customer_final_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date and time components from 'created_ts'\n",
    "enriched_df = customer_final_stream \\\n",
    "    .withColumn('created_year', year(col('created_ts'))) \\\n",
    "    .withColumn('created_month', month(col('created_ts'))) \\\n",
    "    .withColumn('created_day', dayofmonth(col('created_ts'))) \\\n",
    "    .withColumn('created_hour', hour(col('created_ts'))) \\\n",
    "    .withColumn('created_minute', minute(col('created_ts')))\n",
    "\n",
    "# Extract components from 'birthdate' for demographic insights\n",
    "enriched_df = enriched_df \\\n",
    "    .withColumn('birth_year', year(col('birthdate'))) \\\n",
    "    .withColumn('birth_month', month(col('birthdate'))) \\\n",
    "    .withColumn('birth_day', dayofmonth(col('birthdate'))) \n",
    "\n",
    "# Extract join date components from 'first_join_date'\n",
    "enriched_df = enriched_df \\\n",
    "    .withColumn('join_year', year(col('first_join_date'))) \\\n",
    "    .withColumn('join_month', month(col('first_join_date'))) \\\n",
    "    .withColumn('join_day', dayofmonth(col('first_join_date'))) \n",
    "\n",
    "# Define schema for product metadata\n",
    "product_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"product_id\", IntegerType(), True),  # Product ID\n",
    "        StructField(\"quantity\", IntegerType(), True),  # Quantity purchased\n",
    "        StructField(\"item_price\", IntegerType(), True)  # Price per item\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Calculate age based on birth year\n",
    "enriched_df = enriched_df.withColumn('age', year(current_date()) - year(col('birthdate')))\n",
    "\n",
    "# Extract the year from the 'first_join_date'\n",
    "enriched_df = enriched_df.withColumn('first_join_year', year(col('first_join_date')))\n",
    "\n",
    "# Mark if a promo code was used (1 if used, 0 if not)\n",
    "enriched_df = enriched_df.withColumn('promo_code_used', (col('promo_code').isNotNull()).cast('int'))\n",
    "\n",
    "# Parse product metadata and explode it into individual rows for detailed analysis\n",
    "enriched_df = enriched_df \\\n",
    "    .withColumn(\"product_data\", from_json(col(\"product_metadata\"), product_schema)) \\\n",
    "    .withColumn(\"product_exploded\", explode(col(\"product_data\"))) \\\n",
    "    .withColumn(\"product_id\", col(\"product_exploded.product_id\")) \\\n",
    "    .withColumn(\"quantity\", col(\"product_exploded.quantity\")) \\\n",
    "    .withColumn(\"item_price\", col(\"product_exploded.item_price\")) \\\n",
    "    .drop(\"product_exploded\", \"product_data\")  # Drop intermediate columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the streaming data to the console\n",
    "enriched_df_stream = enriched_df.limit(5)\n",
    "customer_final_stream_query = enriched_df_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "customer_final_stream_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used in Model Training:\n",
      "['promo_amount', 'shipment_fee', 'total_amount', 'median_hour', 'age', 'first_join_year', 'created_year', 'created_month', 'created_day', 'created_hour', 'created_minute', 'promo_code_used', 'birth_year', 'birth_month', 'birth_day', 'join_year', 'join_month', 'join_day', 'product_id', 'quantity', 'item_price']\n"
     ]
    }
   ],
   "source": [
    "# Load the saved pipeline model from the specified path\n",
    "model_path = os.getcwd() + '/A2/best_model'  # Adjust path if needed\n",
    "saved_model = PipelineModel.load(model_path)\n",
    "\n",
    "# Initialize VectorAssembler to extract training features\n",
    "vector_assembler = None  \n",
    "for stage in saved_model.stages:  \n",
    "    if isinstance(stage, VectorAssembler):  # Check if the stage is a VectorAssembler\n",
    "        vector_assembler = stage  \n",
    "        break  # Exit the loop once found\n",
    "\n",
    "# Retrieve and print the features used during training\n",
    "if vector_assembler:\n",
    "    training_feature_columns = vector_assembler.getInputCols()  \n",
    "    print(\"Features used in Model Training:\")\n",
    "    print(training_feature_columns)  \n",
    "else:\n",
    "    print(\"VectorAssembler not found in the pipeline model.\")  # Handle case if not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '/A2/best_model'  \n",
    "saved_model = PipelineModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Ensure 'new_time_of_day_index' column exists, initialize with 0 if missing\n",
    "if 'new_time_of_day_index' not in enriched_df.columns:\n",
    "    enriched_df = enriched_df.withColumn('new_time_of_day_index', lit(0))\n",
    "\n",
    "# Ensure 'median_hour' column exists, initialize with 0 if missing    \n",
    "if 'median_hour' not in enriched_df.columns:\n",
    "    enriched_df = enriched_df.withColumn('median_hour', lit(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the streaming data to the console\n",
    "enriched_df_stream = enriched_df.limit(5)\n",
    "customer_final_stream_query = enriched_df_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_final_stream_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- promo_amount: integer (nullable = true)\n",
      " |-- shipment_fee: integer (nullable = true)\n",
      " |-- total_amount: integer (nullable = true)\n",
      " |-- median_hour: integer (nullable = false)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- first_join_year: integer (nullable = true)\n",
      " |-- created_year: integer (nullable = true)\n",
      " |-- created_month: integer (nullable = true)\n",
      " |-- created_day: integer (nullable = true)\n",
      " |-- created_hour: integer (nullable = true)\n",
      " |-- created_minute: integer (nullable = true)\n",
      " |-- promo_code_used: integer (nullable = false)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- birth_month: integer (nullable = true)\n",
      " |-- birth_day: integer (nullable = true)\n",
      " |-- join_year: integer (nullable = true)\n",
      " |-- join_month: integer (nullable = true)\n",
      " |-- join_day: integer (nullable = true)\n",
      " |-- event_ts: timestamp (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- item_price: integer (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- new_time_of_day_index: integer (nullable = false)\n",
      " |-- median_hour: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "required_columns = [\n",
    "    'promo_amount', 'shipment_fee', 'total_amount', 'median_hour', 'age',\n",
    "    'first_join_year', 'created_year', 'created_month', 'created_day',\n",
    "    'created_hour', 'created_minute', 'promo_code_used', 'birth_year',\n",
    "    'birth_month', 'birth_day', 'join_year', 'join_month', 'join_day', 'event_ts',\n",
    "    'product_id', 'quantity', 'item_price', 'payment_status', 'payment_method',\n",
    "    'new_time_of_day_index', 'median_hour'\n",
    "]\n",
    "\n",
    "# Keep only the required columns from the DataFrame to avoid unnecessary data processing\n",
    "filtered_df = enriched_df.select(*required_columns)\n",
    "\n",
    "# Ensure numeric columns are cast to 'int' for consistency\n",
    "filtered_df = filtered_df.select(\n",
    "    *[F.col(c).cast('int') if c in ['promo_amount', 'shipment_fee', 'total_amount'] \n",
    "      else F.col(c) for c in required_columns]\n",
    ")\n",
    "\n",
    "# Double-check the schema to make sure only the necessary columns are present\n",
    "filtered_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.limit(5)\n",
    "fraud_count_query_predictions_df = filtered_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_count_query_predictions_df.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\tThe company is interested in the number of potential frauds as they happen and the products in customersâ€™ shopping carts (so that they can plan their stock level ahead.) Load your ML model, and use the model to predict/process each browsing session/transaction as follows:  \n",
    "a)\tEvery 10 seconds, show the total number of potential frauds (prediction = 1) in the last 2 minutes, and persist the raw data (see 7a).  \n",
    "b)\tEvery 30 seconds, find the top 20 products (order by quantity descending) in the last 30 seconds, show product ID, name and total quantity. We only need the non-fraud transactions (prediction=0) by extracting customer shopping cart details (sum of all items of ADD_TO_CART(ATC) events from browsing behaviour, you can also extract it from transactions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pre-trained model to make predictions on the filtered streaming data\n",
    "predictions_df = saved_model.transform(filtered_df)\n",
    "\n",
    "# Filter out potential fraud cases (where prediction value is 1)\n",
    "potential_frauds_df = predictions_df.filter(col(\"gbt_prediction\") == 1)\n",
    "\n",
    "# Aggregate potential frauds in the last 2 minutes, updating every 10 seconds\n",
    "fraud_count_df = potential_frauds_df \\\n",
    "    .groupBy(window(col(\"event_ts\"), \"2 minutes\", \"10 seconds\")) \\\n",
    "    .count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = predictions_df.limit(2)\n",
    "fraud_count_query_predictions_df = predictions_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_count_query_predictions_df.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_count_query = fraud_count_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_count_query.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the saved model to the filtered data to generate predictions\n",
    "predictions_df = saved_model.transform(filtered_df)\n",
    "\n",
    "# Filter non-fraud transactions (where prediction = 0) and focus on 'ADD_TO_CART' events\n",
    "non_fraud_df = predictions_df.filter((col(\"gbt_prediction\") == 0) & (col(\"event_type\") == \"ATC\"))\n",
    "\n",
    "# Identify the top 20 products in the last 30 seconds using a tumbling window\n",
    "top_products_df = non_fraud_df \\\n",
    "    .withWatermark(\"event_ts\", \"2 minutes\") \\\n",
    "    .groupBy(window(col(\"event_ts\"), \"30 seconds\"), \"product_id\") \\\n",
    "    .agg(spark_sum(\"quantity\").alias(\"total_quantity\")) \\\n",
    "    .limit(20)  # Limiting results to the top 20 products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 11: Output top 20 products to the console every 30 seconds\n",
    "top_products_query = top_products_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\tWrite a Parquet file and save the following data frames (tip: you may look at part 3 and think about what columns to save):  \n",
    "a.\tPersist the raw data from 6a in parquet format. Every student may have different features/columns in their data frames depending on their model, at the bare minimum, we need some IDs to identify those frauds later on (transaction_id and/or session_id). After that, read the parquet file and show a few rows to verify it is saved correctly.  \n",
    "b.\tPersist the data from 6b in another parquet file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(df, batch_id):\n",
    "    # Save the DataFrame as a Parquet file, appending each batch to the existing data\n",
    "    df.write.parquet(f\"{os.getcwd()}/A2_B/fraud_raw_data_1.parquet\", mode=\"append\")\n",
    "\n",
    "# Persist the raw fraud data to Parquet every 10 seconds using foreachBatch\n",
    "raw_fraud_query = potential_frauds_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(save_to_parquet) \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_fraud_query.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet_b(df, batch_id):\n",
    "    # Save the DataFrame as a Parquet file, appending each batch to the existing data\n",
    "    df.write.parquet(f\"{os.getcwd()}/A2_B/top_products_1.parquet\", mode=\"append\")\n",
    "\n",
    "# Persist the top products data to Parquet every 10 seconds using foreachBatch\n",
    "top_products_persist_query = top_products_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(save_to_parquet_b) \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products_persist_query.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\tRead the two parquet files from task 7 as data streams and send to Kafka topics with appropriate names.\n",
    "(Note: You shall read the parquet files as a streaming data frame and send messages to the Kafka topic when new data appears in the parquet file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
